{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMDQgvmeLgCc",
        "colab_type": "text"
      },
      "source": [
        "## Implementation details :\n",
        "\n",
        "1) Using MLP for classifying MNIST digits. a. Let us consider a MLP network with H hidden units, O outputs, and\n",
        "inputs of size D.\n",
        "\n",
        "2) According to above diagram dimensions of network will be :\n",
        "```\n",
        " W - H * D+1   -- Weights from Input layer to Hidden layer\n",
        " V - K * H+1   -- Weights from Hidden layer to Output layer\n",
        " X - N * D+1   -- Weights of Input data\n",
        " Y - N * K     -- Weights of Output label data\n",
        " Z - H+1 * 1   -- Hidden layer Weights\n",
        " O - K * 1     -- Output layer weights\n",
        "```\n",
        "3) Please note that +1 in above notations is to indicate bias term.\n",
        "\n",
        "4) tanh is used as the activation function.\n",
        "\n",
        "5) During forward pass will be :\n",
        "\n",
        "$\\mathbf{z}=\\tanh (\\mathbf{W} \\mathbf{x})$\n",
        "\n",
        "and $O_{i}=\\frac{\\exp v_{i}^{T} z}{\\sum_{k=1}^{K} \\exp v_{k}^{T} z}$\n",
        "\n",
        "Overall loss function will be :\n",
        "\n",
        "Total loss = $-\\sum_{n=1}^{N} \\sum_{i=1}^{K} y_{n i} \\log O_{n i}$\n",
        "\n",
        "\n",
        "The dataset is included in zip file (\"data.txt\" and \"label.txt\"). Number of hidden layer units to be 500, learning rate is set to be 0.01. \n",
        "\n",
        " Inorder to update the weights during back propogation we will modified version of stochastic gradient descent, where instead of updating weights after each data point, the updates are made once with batch of input data, Let batch size = 25. Number of epochs = 100.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTqHlDiTLez4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "####################################################################################\n",
        "# Derivative of weights as defined in Lab3.a\t####################################\n",
        "####################################################################################\n",
        "####################################################################################\n",
        "\n",
        "# Dimension of all the Matrices Used\n",
        "# W - H * D+1   -- Weights from Input layer to Hidden layer\n",
        "# V - K * H+1   -- Weights from Hidden layer to Output layer\n",
        "# X - N * D+1   -- Weights of Input data\n",
        "# Y - N * K     -- Weights of Output label data\n",
        "# Z - H+1 * 1   -- Hidden layer Weights\n",
        "# O - K * 1     -- Output layer weights\n",
        "\n",
        "\n",
        "# Function to read data and store it in form of 2D array\n",
        "def read_data(file_name) :\n",
        "    data = np.loadtxt(file_name, delimiter=',')\n",
        "    return data\n",
        "\n",
        "\n",
        "# Implementation of Forward pass function\n",
        "def forward_pass(data_points,W,V,Y) :\n",
        "    # use compute_Z_values, compute_O_values, compute_softmax, calculate_error to compute error, O_softmax\n",
        "    # During forward pass compute Z values, O_values, O_softmax, error\n",
        "    # Insert code here\n",
        "#     print(\"reaching here\")\n",
        "    Z_values = compute_Z_values(W,data_points) \n",
        "    O_values = compute_O_values(V,Z_values)\n",
        "    O_softmax = compute_softmax(O_values)\n",
        "\n",
        "    error = calculate_error(O_softmax,Y)\n",
        "    #return the error, O_softmax\n",
        "    return error,O_softmax\n",
        "\n",
        "\n",
        "# Implementation of Cross Entropy Error Function takes as input O_softmax, Y\n",
        "def calculate_error(predictions, targets, epsilon=1e-10):\n",
        "    # Caculate cross entropy error between output of softmax (predictions) , actual values (targets)\n",
        "    #Insert code here\n",
        "    #predictions is 25*10\n",
        "    #targets is 25*10\n",
        "    logOfY = np.log(predictions)\n",
        "#     logOfoneminusY = np.log(predictions*(-1)+1)\n",
        "    sumoftwoterms = np.multiply(targets,logOfY)   # + np.multiply(targets(-1)+1,logOfoneminusY)\n",
        "    cross_entropy_error = np.sum(sumoftwoterms,axis=1)\n",
        "    #returns cross entropy error\n",
        "    cross_entropy_error = (-1)*np.mean(cross_entropy_error,axis=0)\n",
        "    return cross_entropy_error\n",
        "\n",
        "\n",
        "# Implementation of Softmax Error Function takes as input O\n",
        "def compute_softmax(output_matrix) :\n",
        "    # return output_matrix after apply softmax function ( hint: use np.exp function )\n",
        "    y_length = 10\n",
        "    #Insert code here\n",
        "#     expo = np.exp(output_matrix)\n",
        "#     rowsum = np.sum(expo,axis=1)                # it is 25x1\n",
        "# #     print(\"rowsum shape=\")\n",
        "# #     print(rowsum.shape)\n",
        "#     for i in range(0,10):      \n",
        "#       expo[i] = np.divide(expo[i],rowsum[i])\n",
        "    n,m = output_matrix.shape\n",
        "    output_matrix = output_matrix.astype(float)\n",
        "    for i in range(n):\n",
        "        output_matrix[i] = np.exp(output_matrix[i])/np.sum(np.exp(output_matrix[i]),axis=0)\n",
        "    # returns output_matrix\n",
        "    return output_matrix\n",
        "\n",
        "\n",
        "# Implementation of Backward pass using Backpropagation Algorithm to calculate V_new, W_new, bias_v\n",
        "def backward_pass(O_softmax,Y,V,Z,W,X,bias_z):\n",
        "    # use gradient_hidden_to_output, gradient_input_to_hidden functions to compute V_new, bias_v, W_new \n",
        "    #Insert code here   \n",
        "    V_new,bias_v = gradient_hidden_to_output(O_softmax,Y,Z,bias_z)\n",
        "    W_new = gradient_input_to_hidden(O_softmax,Y,V,Z,X)    #501*401\n",
        "    W_new = np.delete( W_new ,500,0)   #500*400         \n",
        "    #returns V_new, W_new, bias_v\n",
        "    return  W_new, V_new, bias_v\n",
        "\n",
        "\n",
        "#Implementation of Gradient back propogation from Hidden to output Layer\n",
        "def gradient_hidden_to_output(O_softmax,Y,Z,bias_z) :\n",
        "    # function to update V values using backpropogation using matrix operations.\n",
        "    #Insert code here\n",
        "\n",
        "    gradL=np.subtract(O_softmax,Y)  #25*10\n",
        "    final_result_matrix=np.dot(np.transpose(gradL),Z)    \n",
        "    bias_v=np.dot(np.transpose(gradL),bias_z)\n",
        "    \n",
        "    return final_result_matrix,bias_v  #final_result is V_new or basically V_grad or dL/dV\n",
        "\n",
        "#Implementation of Graident back propogation from Input to Hidden Layer\n",
        "def gradient_input_to_hidden(O_softmax,Y,V,Z,X) :\n",
        "    # function to update W values using backpropogation using only matrix operations.\n",
        "    #Insert code here\n",
        "    temp  = np.transpose((np.dot(np.subtract(O_softmax,Y),V)))  #25*501\n",
        "#     print(\"Z.shape()\")\n",
        "#     print(Z.shape)\n",
        "    sech2 = (-1)*np.square(Z)+1  #25*501\n",
        "    result_matrix  = np.dot(np.multiply(temp,np.transpose(sech2)), X)  #501*25 x 25*401\n",
        "    # returns updated W values   #final_result is W_new or basically W_grad\n",
        "    return result_matrix  #501*401\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to calculate Z values during forward pass\n",
        "def compute_Z_values(weights,data_points) :\n",
        "    # function to update Z during forward pass using matrix operations.\n",
        "    #Insert code here\n",
        "    \n",
        "    z_values = np.tanh(np.dot(data_points,np.transpose(weights)))  #arr1_transpose = arr1.transpose() \n",
        "    #return calculated z_values\n",
        "    z_values = np.append(np.ones((len(z_values),1)),z_values,axis=1)\n",
        "    return z_values\n",
        "\n",
        "\n",
        "# Function to Calculate output matrix during forward pass\n",
        "def compute_O_values(weights,z_values) :\n",
        "    # function to update O during forward pass using matrix operations.\n",
        "    #Insert code here\n",
        "    \n",
        "#     print(\"weights.shape this is\")    \n",
        "#     print(weights.shape)\n",
        "#     print(\"z_values.shape\")\n",
        "#     print(z_values.shape)\n",
        "    \n",
        "    o_values = np.dot(z_values,np.transpose(weights))   #O is 25x10     \n",
        "    #return calculated o_values\n",
        "    #return o_values.T    \n",
        "    return o_values\n",
        "\n",
        "\n",
        "\n",
        "# Function to Intialise weights with bias term\n",
        "def initilaise_weights(data) :\n",
        "    # function to append bias term.\n",
        "    # insert code here\n",
        "    r = data.shape[0]\n",
        "#     o = np.ones(r,1)\n",
        "#     final_data = np.hstack((o, data ))\n",
        "#     return final_data\n",
        "    bias_data = np.empty(shape=(r, 1))\n",
        "    bias_data.fill(1.0)\n",
        "    data = np.append(bias_data, data ,axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "# To intiliase random weights to Matrices such as W, V\n",
        "def random_weights(number_of_rows,number_of_columns) :\n",
        "    # Function to assign random weights to W, V\n",
        "    #Insert code here \n",
        "    new_data = np.random.randn(number_of_rows,number_of_columns)-0.5\n",
        "  \n",
        "    # return random weights with number_of_rows * number_of_columns from normal distribution\n",
        "    return new_data\n",
        "\n",
        "\n",
        "# To divide the data into test train data\n",
        "def train_test1_split(X,Y,fraction) :\n",
        "    # Function to divide train, validation and test data based on fraction. let fraction = 0.8 then train = 0.75, validation= 0.05 and test = 0.2 \n",
        "    #Insert code here\n",
        "    data_train_x, test_data_x, data_train_y, test_data_y = train_test_split(X,Y, test_size=1-fraction, random_state=1)      # test_set gets (100-fraction*100)% of X,Y rest goes to train \n",
        "\n",
        "    data_train_x, validation_data_x, data_train_y, validation_data_y = train_test_split(data_train_x, data_train_y , test_size=0.2, random_state=1)  # (100-fraction*100)% of trainset goes to validation and rest to train set\n",
        "    \n",
        "\n",
        "    # return data_train_x,data_train_y,validation_data_x,validation_data_y,test_data_x,test_data_y\n",
        "    return data_train_x,data_train_y,validation_data_x,validation_data_y,test_data_x,test_data_y\n",
        "\n",
        "\n",
        "# Shuffle in same order for X,Y\n",
        "def shuffle(a, b, seed):\n",
        "   # to Shuffle in same order for X,Y based on seed\n",
        "   #Insert code here\n",
        "    \n",
        "    zipped = list(zip(a,b))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(zipped)\n",
        "    a,b=zip(*zipped)\n",
        "   # return shuffled values a,b in same order\n",
        "    return a,b\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    data = read_data(\"data.txt\")\n",
        "    Y = read_data(\"label.txt\")\n",
        "    X = initilaise_weights(data)\n",
        "    W = random_weights(500,401)\n",
        "    V = random_weights(10,501)\n",
        "    Z = compute_Z_values(W,X[:25,:])\n",
        "    \n",
        "    \n",
        "#     print(Z.shape)\n",
        "     \n",
        "    O = compute_O_values(V,Z)\n",
        "    bias_z = np.empty(shape=(25, 1))\n",
        "    bias_z.fill(1.0)\n",
        "    i=0\n",
        "    learning_rate = 0.01\n",
        "    train_test_fraction  = 0.8\n",
        "    train_validation_split = 0.2\n",
        "    train_data_x,train_data_y,validation_data_x,validation_data_y,test_data_x,test_data_y =train_test1_split(X,Y,train_test_fraction)\n",
        "    number_of_epocs=100\n",
        "    train_error_epoch = []*(5*number_of_epocs)\n",
        "    #X = train_data_x\n",
        "    #Y = train_data_y\n",
        "    #Dividing the data into training data and test data into 0.8 ratio same ration for train and validation data,values below correspond to 0.8 ratio\n",
        "    train_data_len = 3200\n",
        "    validation_data_len = 800\n",
        "    test_data_len =1000\n",
        "    validation_error_epoch = [] * (5*number_of_epocs)\n",
        "    # Running for 5 trails using 100 Epocs and Batch size = 25\n",
        "    batch_size = 25\n",
        "    print(\"Started calculating Training error in 5 Trails for each epoch with Batch size = 25 \")\n",
        "    print(\"Started calculating Validation error in 5 Trails for each epoch with Batch size = 25 \")\n",
        "    # Different trails are performed for 5 times.\n",
        "    # 5 different trails\n",
        "    for k in range(5) :\n",
        "        W = random_weights(500, 401)\n",
        "        V = random_weights(10, 501)\n",
        "        error_train=0\n",
        "        error_validation = 0\n",
        "        # Randomising the data\n",
        "        seed = random.randint(10000,10000000)\n",
        "        \n",
        "        X,Y = shuffle(X,Y,seed)\n",
        "        X = np.array(X)\n",
        "        Y = np.array(Y)\n",
        "#         print(X.shape)\n",
        "#         print(Y.shape)\n",
        "        print(\"Training Error for 100th epoch for Trail Number : \"+str(k+1))\n",
        "        for j in range(number_of_epocs) :\n",
        "            i=0\n",
        "            count=0\n",
        "            error_train = 0.0\n",
        "            error_validation = 0.0\n",
        "            #X,Y = shuffle(X,Y,12345)\n",
        "            while i < (train_data_len)  :\n",
        "                i1=i\n",
        "                # Batch size is 25\n",
        "                i= i+25\n",
        "                #error,O_softmax = forward_pass(np.array(X)[i1:i, :], W, V, np.array(Y)[i1:i, :])\n",
        "                \n",
        "                error,O_softmax = forward_pass(X[i1:i, :], W, V, Y[i1:i, :])  \n",
        "                W_new,V_new,bias_v=backward_pass(O_softmax,Y[i1:i,:],V,Z,W,X[i1:i,:],bias_z)\n",
        "                print(\"W_new is = \",W_new)\n",
        "                #print(W_new.shape)\n",
        "                W = W - (learning_rate/25)*W_new\n",
        "                #print(W)\n",
        "                V_new=np.delete(V_new,500,1)                             #deleting bias weights\n",
        "                V_new = np.append(learning_rate*bias_v,V_new,axis=1)\n",
        "                V = V - (learning_rate/25)*V_new\n",
        "                error_train+= error\n",
        "                count+=1\n",
        "                #print(error1)\n",
        "                #print(V.shape)\n",
        "            #print(error)\n",
        "            #print(j)\n",
        "            error, O_softmax = forward_pass(X[0:3200, :], W, V, Y[0 : 3200, :])\n",
        "            error1, O_softmax = forward_pass(X[3200:4000, :], W, V, Y[3200:4000, :])\n",
        "            error_validation = error1\n",
        "            count = train_data_len/batch_size\n",
        "            count1= validation_data_len/batch_size\n",
        "            error_train = error_train/count\n",
        "            error_validatio = error_validation/count1\n", 
        "            print(\"Training error after  epoch : \"+str(j+1)+\" here every batch size = 25\")\n",
        "            print(error_train)\n",
        "            print(\"Validation error after  epoch : \"+str(j+1)+\" here batch size = 25\")\n",
        "            print(error_validation)\n",
        "            train_error_epoch.append(error_train)\n",
        "            validation_error_epoch.append(error_validation)\n",
        "    print(\"\\n\")\n",
        "    print(\"Final Training Errors after 5 trails and 100 Epocs : \")\n",
        "    print(train_error_epoch)\n",
        "    print(\"\\n\")\n",
        "    print(\"Final Validation Errors after 5 trails and 100 Epocs : \")\n",
        "    print(validation_error_epoch)\n",
        "    mean_training = []\n",
        "    variance_training = []\n",
        "    mean_validation = []\n",
        "    variance_validation = []\n",
        "    train_error_epoch = np.reshape(train_error_epoch,(5,number_of_epocs))\n",
        "    validation_error_epoch = np.reshape(validation_error_epoch,(5,number_of_epocs))\n",
        "    mean_training = np.mean(train_error_epoch, axis=0)\n",
        "    mean_validation = np.mean(validation_error_epoch,axis=0)\n",
        "    variance_training = np.var(train_error_epoch,axis=0)\n",
        "    variance_validation = np.var(validation_error_epoch,axis=0)\n",
        "    mean_training = np.reshape(mean_training,(number_of_epocs,))\n",
        "    mean_validation = np.reshape(mean_validation, (number_of_epocs))\n",
        "    variance_training = np.reshape(variance_training,(number_of_epocs,))\n",
        "    variance_validation = np.reshape(variance_validation,(number_of_epocs,))\n",
        "    epochs = []\n",
        "    for i in range(1,number_of_epocs+1) :\n",
        "        epochs.append(i)\n",
        "    print(\"\\n\")\n",
        "    print(\"Plots are started to Generate In Figures Folder : \")\n",
        "    plt.plot(epochs,mean_training, color='red', label='Training')\n",
        "    plt.xlabel(\"Epoch values\")\n",
        "    #plt.title(\"Plot for Training Error Vs Epochs\")\n",
        "    location = \"./figures/lab3.a_TrainingError\" + \".png\"\n",
        "    #plt.savefig(location)\n",
        "    #plt.close()\n",
        "    plt.plot(epochs, mean_validation,color='blue', label='Validation')\n",
        "    plt.ylabel(\"Training,Validation Error values\")\n",
        "    plt.title(\"Plot for Training,Validation Error Vs Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    location = \"./figures/lab3.a_TrainingAndValidationError\" + \".png\"\n",
        "    plt.savefig(location)\n",
        "    plt.close()\n",
        "    #plt.ylim(0.0145,0.01465)\n",
        "    plt.plot(epochs, mean_training, color='red', label='Training')\n",
        "    plt.xlabel(\"Epoch values\")\n",
        "    plt.ylabel(\"Mean Training Error values\")\n",
        "    plt.title(\"Plot for Mean Training Error Vs Epochs\")\n",
        "    location = \"./figures/lab3.a_MeanTrainingError\" + \".png\"\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig(location)\n",
        "    plt.close()\n",
        "    #plt.ylim(0.0133, 0.134)\n",
        "    plt.plot(epochs, mean_validation, color='blue', label='Validation')\n",
        "    plt.xlabel(\"Epoch values\")\n",
        "    plt.ylabel(\" Mean Validation Error values\")\n",
        "    plt.title(\"Plot for Mean Validation Error Vs Epochs\")\n",
        "    location = \"./figures/lab3.a_MeanValidationError\" + \".png\"\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig(location)\n",
        "    plt.close()\n",
        "    #plt.ylim(0.000240, 0.000242)\n",
        "    plt.plot(epochs, variance_training, color='red', label='Training')\n",
        "    plt.xlabel(\"Epoch values\")\n",
        "    plt.ylabel(\" Variance Training Error values\")\n",
        "    plt.title(\"Plot for Variance Training Error Vs Epochs\")\n",
        "    location = \"./figures/lab3.a_VarianceTrainingError\" + \".png\"\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig(location)\n",
        "    plt.close()\n",
        "    #plt.ylim(0.000210, 0.000211)\n",
        "    plt.plot(epochs, variance_validation, color='blue', label='Validation')\n",
        "    plt.xlabel(\"Epoch values\")\n",
        "    plt.ylabel(\"Variance Validation Error values\")\n",
        "    plt.title(\"Plot for Variance Validation Error Vs Epochs\")\n",
        "    location = \"./figures/lab3.a_VarianceValidationError\" + \".png\"\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig(location)\n",
        "    plt.close()\n",
        "    print(\"\\n\")\n",
        "    print(\"Plots are Generated Successfully In Figures folder\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
